easyblock = 'Tarball'

name = 'Spark'
version = '2.3.1'

# Currently not needed due to the change to osdependencies
#javaver = 'jdk1.8.0_51'

versionsuffix = '-Hadoop-2.7'

homepage = 'http://spark.apache.org'
description = """Spark is Hadoop MapReduce done in memory"""

toolchain = {'name': 'CrayGNU', 'version': '19.03'}

patches = [('spark-2.3.1-Hadoop-2.7.patch', 1)]

sources = ['%(namelower)s-%(version)s-bin-hadoop2.7.tgz']
source_urls = [
    'http://apache.belnet.be/%(namelower)s/%(namelower)s-%(version)s/',
    'http://www.eu.apache.org/dist/%(namelower)s/%(namelower)s-%(version)s/',
    'http://www.us.apache.org/dist/%(namelower)s/%(namelower)s-%(version)s/',
]

checksums = ['e87499e5417a64341cbda25e087632dd9f6ce7ad249dfeba47d9d02a51305fc2']

# On CLE7, java is no longer avail. as a stand-alone module
# Therefore, `osdependencies` are used
#dependencies = [
#    ('java/%s' % javaver, EXTERNAL_MODULE),
#]
osdependencies = ['java']

postinstallcmds = [
    ("chmod +x %(installdir)s/cscs-custom/getmaster.sh"),
    ("chmod +x %(installdir)s/cscs-custom/getslaves.sh"),
    ("chmod +x %(installdir)s/cscs-custom/start-all.sh"),
    ("chmod +x %(installdir)s/cscs-custom/start-ssh.sh"),
    ("chmod +x %(installdir)s/cscs-custom/stop-all.sh")]

sanity_check_paths = {
    'files': ['bin/spark-shell', 'bin/pyspark'],
    'dirs': ['python']
}

moduleclass = 'devel'

modtclfooter = """
## Required internal variables
set name     spark
set version  %(version)s
set sparkbin  $root/cscs-custom

## Required for SVN hook to generate SWDB entry
set     fullname    Spark
set     externalurl http://spark.apache.org
set     nerscurl    http://spark.apache.org
set     maincategory    applications
set     subcategory debugging
set     description "Spark data analytic framework"

prepend-path PATH $sparkbin:$root
if {! [info exists env(SLURM_JOBID) ] } {
  puts stderr "SLURM_JOBID not set, please run this module inside of a batch job"
  exit
}

set workerdir $env(PWD)/sparkjob.$env(SLURM_JOBID)
set nodefile $env(SLURM_JOB_NODELIST)

setenv SPARK_WORKER_DIR $workerdir
setenv SPARK_SLAVES $workerdir/slaves
setenv SPARK_LOG_DIR $workerdir/sparklogs
setenv SPARK_LOCAL_DIRS "/tmp"
setenv SPARK_PREFIX $root
setenv SPARK_CONF_DIR $workerdir/conf

if { [ module-info mode load ] } {
    puts stderr "Creating Directory SPARK_WORKER_DIR $env(SPARK_WORKER_DIR)"
    puts stderr "Creating $env(SPARK_WORKER_DIR)/slaves file"
    puts stderr "Determining the master node name..."
    set master [exec $sparkbin/getmaster.sh]
    puts stderr "Master node is $master"

    exec /bin/mkdir -p $env(SPARK_WORKER_DIR)
    exec  $sparkbin/getslaves.sh $env(SPARK_WORKER_DIR)/slaves
    setenv SPARKURL spark://$master:7077
    setenv SPARKMASTER $master
}
"""
