easyblock = 'Tarball'

name = 'Spark'
version = '2.4.7'
versionsuffix = '-Hadoop-2.7'

homepage = 'http://spark.apache.org'
description = "Spark is Hadoop MapReduce done in memory"

toolchain = {'name': 'CrayGNU', 'version': '21.09'}

source_urls = ['https://downloads.apache.org/%(namelower)s/%(namelower)s-%(version)s/']
sources = ['%(namelower)s-%(version)s-bin-hadoop2.7.tgz']
patches = [('%(namelower)s-%(version)s-Hadoop-2.7.patch', 1)]
checksums = ['13098490936c9931beda3acc4c30cdc5ca707acd1415eebde1030b11903934fe']

osdependencies = ['java']

postinstallcmds = [
    "chmod +x %(installdir)s/cscs-custom/getmaster.sh",
    "chmod +x %(installdir)s/cscs-custom/getslaves.sh",
    "chmod +x %(installdir)s/cscs-custom/start-all.sh",
    "chmod +x %(installdir)s/cscs-custom/start-ssh.sh",
    "chmod +x %(installdir)s/cscs-custom/stop-all.sh",
]
modtclfooter = """
## Required internal variables
set name     %(namelower)s
set version  %(version)s
set sparkbin  $root/cscs-custom

## Required for SVN hook to generate SWDB entry
set     fullname    %(name)s
set     externalurl http://%(namelower)s.apache.org
set     nerscurl    http://%(namelower)s.apache.org
set     maincategory    applications
set     subcategory debugging
set     description "%(name)s data analytic framework"

prepend-path PATH $sparkbin:$root
if {! [info exists env(SLURM_JOBID) ] } {
  puts stderr "SLURM_JOBID not set, please run this module inside of a batch job"
  exit
}

set workerdir $env(PWD)/sparkjob.$env(SLURM_JOBID)
set nodefile $env(SLURM_JOB_NODELIST)

setenv SPARK_WORKER_DIR $workerdir
setenv SPARK_SLAVES $workerdir/slaves
setenv SPARK_LOG_DIR $workerdir/sparklogs
setenv SPARK_LOCAL_DIRS "/tmp"
setenv SPARK_PREFIX $root
setenv SPARK_CONF_DIR $workerdir/conf

if { [ module-info mode load ] } {
    puts stderr "Creating Directory SPARK_WORKER_DIR $env(SPARK_WORKER_DIR)"
    puts stderr "Creating $env(SPARK_WORKER_DIR)/slaves file"
    puts stderr "Determining the master node name..."
    set master [exec $sparkbin/getmaster.sh]
    puts stderr "Master node is $master"

    exec /bin/mkdir -p $env(SPARK_WORKER_DIR)
    exec  $sparkbin/getslaves.sh $env(SPARK_WORKER_DIR)/slaves
    setenv SPARKURL %(namelower)s://$master:7077
    setenv SPARKMASTER $master
}
"""

sanity_check_paths = {
    'files': ['bin/%(namelower)s-shell', 'bin/pyspark'],
    'dirs': ['python'],
}

moduleclass = 'devel'
