easyblock = 'Tarball'

name = 'Spark'
version = '2.2.0'

javaver = 'jdk1.8.0_51'

versionsuffix = '-Hadoop-2.6'

homepage = 'http://spark.apache.org'
description = """Spark is Hadoop MapReduce done in memory"""

toolchain = {'name': 'CrayGNU', 'version': '17.08'}

patches = [('spark-2.2.0-Hadoop-2.6.patch', 1)]

sources = ['%(namelower)s-%(version)s-bin-hadoop2.6.tgz']
source_urls = [
    'http://apache.belnet.be/%(namelower)s/%(namelower)s-%(version)s/',
    'http://www.eu.apache.org/dist/%(namelower)s/%(namelower)s-%(version)s/',
    'http://www.us.apache.org/dist/%(namelower)s/%(namelower)s-%(version)s/',
]
checksums = ['86f33f6f4fe545cb848600004144f09b6e8413af47ce02c1203c3ecda075288d']

dependencies = [
    ('java/%s' % javaver, EXTERNAL_MODULE),
]

postinstallcmds = [
    ("chmod +x %(installdir)s/cscs-custom/getmaster.sh"),
    ("chmod +x %(installdir)s/cscs-custom/getslaves.sh")]

sanity_check_paths = {
    'files': ['bin/spark-shell', 'bin/pyspark'],
    'dirs': ['python']
}

moduleclass = 'devel'

modtclfooter = """
## Required internal variables
set name     spark
set version  %(version)s
set sparkbin  $root/cscs-custom 

## Required for SVN hook to generate SWDB entry
set     fullname    Spark
set     externalurl http://spark.apache.org
set     nerscurl    http://spark.apache.org
set     maincategory    applications
set     subcategory debugging
set     description "Spark data analytic framework"

prepend-path PATH $sparkbin:$root
if {! [info exists env(SLURM_JOBID) ] } {
  puts stderr "SLURM_JOBID not set, please run this module inside of a batch job"
  exit
}

set workerdir $env(PWD)/sparkjob.$env(SLURM_JOBID)
set nodefile $env(SLURM_JOB_NODELIST)

setenv SPARK_WORKER_DIR $workerdir
setenv SPARK_SLAVES $workerdir/slaves
setenv SPARK_LOG_DIR $workerdir/sparklogs
setenv SPARK_LOCAL_DIRS "/tmp"
setenv SPARK_PREFIX $root
setenv SPARK_CONF_DIR $workerdir/conf

if { [ module-info mode load ] } {
    puts stderr "Creating Directory SPARK_WORKER_DIR $env(SPARK_WORKER_DIR)"
    puts stderr "Creating $env(SPARK_WORKER_DIR)/slaves file"
    puts stderr "Determining the master node name..."
    set master [exec $sparkbin/getmaster.sh]
    puts stderr "Master node is $master"

    exec /bin/mkdir -p $env(SPARK_WORKER_DIR)
    exec  $sparkbin/getslaves.sh $env(SPARK_WORKER_DIR)/slaves
    setenv SPARKURL spark://$master:7077
    setenv SPARKMASTER $master
}
"""
