diff --git a/tensorflow/workspace.bzl b/tensorflow/workspace.bzl
--- a/tensorflow/workspace.bzl    2017-08-17 03:20:31.000000000 +0200
+++ b/tensorflow/workspace.bzl    2017-10-10 11:27:09.031319000 +0200
@@ -330,7 +330,8 @@
           "https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz",
           "http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz",
       ],
-      sha256 = "6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93",
+      # sha256 = "6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93",
+      sha256 = "e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d",
       strip_prefix = "protobuf-0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66",
       # TODO: remove patching when tensorflow stops linking same protos into
       #       multiple shared libraries loaded in runtime by python.
diff --git a/configure-cscs.sh b/configure-cscs.sh
new file mode 100755
index 0000000..11f0357
--- /dev/null
+++ b/configure-cscs.sh
@@ -0,0 +1,37 @@
+#!/bin/bash
+
+sed -i s\@'$GCC_PATH'@"$GCC_PATH"@g third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl
+sed -i s\@'$CRAY_CUDATOOLKIT_DIR'@"$CRAY_CUDATOOLKIT_DIR"@g third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl
+
+sed -i s\@'$GCC_PATH'@"$GCC_PATH"@g third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl
+sed -i s\@'$CRAY_CUDATOOLKIT_DIR'@"$CRAY_CUDATOOLKIT_DIR"@g third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl
+
+CONF_IN="configure.in"+
+
+echo "$EBROOTPYTHON/bin/python"$PYMAJVER > $CONF_IN
+echo "$EBROOTPYTHON/lib/python"$PYMAJVER"."$PYMINVER"/site-packages" >> $CONF_IN
+echo "y" >> $CONF_IN # Do you wish to use jemalloc as the malloc implementation? [Y/n] y
+echo "n" >> $CONF_IN # Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
+echo "n" >> $CONF_IN # Do you wish to build TensorFlow with Hadoop File System support? [y/N]
+echo "n" >> $CONF_IN # Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]:
+echo "n" >> $CONF_IN # Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]
+echo "n" >> $CONF_IN # Do you wish to build TensorFlow with GDR support? [y/N]:
+echo "n" >> $CONF_IN # Do you wish to build TensorFlow with VERBS support? [y/N]
+echo "n" >> $CONF_IN # Do you wish to build TensorFlow with OpenCL support? [y/N] n
+echo "y" >> $CONF_IN # Do you want CUDA support? [y/N] y
+echo "8.0" >> $CONF_IN # Please specify the CUDA SDK version you want to use
+echo $CRAY_CUDATOOLKIT_DIR >> $CONF_IN # Please specify the CUDA SDK version you want to use
+echo "7" >> $CONF_IN # Please specify the cuDNN version you want to use.
+echo $EBROOTCUDNN >> $CONF_IN # Please specify the location where cuDNN 5 library is installed.
+echo "6.0" >> $CONF_IN # Please specify a list of comma-separated Cuda compute capabilities you want to build with
+echo "n" >> $CONF_IN # Do you want to use clang as CUDA compiler? [y/N] n
+echo $GCC_PATH"/bin/gcc" >> $CONF_IN # Please specify which gcc should be used by nvcc as the host compiler 
+echo "n" >> $CONF_IN # MPI support?
+echo "-march=native" >> $CONF_IN # Please specify optimization flags "--config=opt" :
+
+
+cat $CONF_IN
+echo "---"
+./configure < $CONF_IN
+
+exit
diff --git a/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl  b/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl 
--- a/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl    2017-07-18 15:47:11.062589066 +0200
+++ b/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl    2017-06-30 03:34:24.000000000 +0200
@@ -44,7 +44,7 @@

   tool_path { name: "ar" path: "/usr/bin/ar" }
   tool_path { name: "compat-ld" path: "/usr/bin/ld" }
-  tool_path { name: "cpp" path: "/usr/bin/cpp" }
+  tool_path { name: "cpp" path: "$GCC_PATH/snos/bin/cpp" }
   tool_path { name: "dwp" path: "/usr/bin/dwp" }
   # As part of the TensorFlow release, we place some cuda-related compilation
   # files in @local_config_cuda//crosstool/clang/bin, and this relative
@@ -59,7 +59,12 @@
   linker_flag: "-B/usr/bin/"

 %{host_compiler_includes}
-  tool_path { name: "gcov" path: "/usr/bin/gcov" }
+  tool_path { name: "gcov" path: "$GCC_PATH/snos/bin/gcov" }
+
+  cxx_builtin_include_directory: "$GCC_PATH/snos/include"
+  cxx_builtin_include_directory: "$GCC_PATH/snos/lib/gcc/x86_64-suse-linux/default/include-fixed/"
+  cxx_builtin_include_directory: "$GCC_PATH/snos/lib/gcc/x86_64-suse-linux/default/include/"
+  cxx_builtin_include_directory: "$CRAY_CUDATOOLKIT_DIR/include/"

   # C(++) compiles invoke the compiler (as that is the one knowing where
   # to find libraries), but we provide LD so other rules can invoke the linker.
@@ -119,6 +124,7 @@
   # Gold linker only? Can we enable this by default?
   # linker_flag: "-Wl,--warn-execstack"
   # linker_flag: "-Wl,--detect-odr-violations"
+  linker_flag: "-Wl,-rpath,$GCC_PATH/snos/lib64/"

   # Include directory for cuda headers.
 %{cuda_include_path}
diff --git a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl
index b7d6cc6..8dfb52e 100755
--- a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl
+++ b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl
@@ -46,11 +46,14 @@
 import pipes

 # Template values set by cuda_autoconf.
-CPU_COMPILER = ('%{cpu_compiler}')
-GCC_HOST_COMPILER_PATH = ('%{gcc_host_compiler_path}')
+CPU_COMPILER = ('$GCC_PATH/snos/bin/gcc')
+GCC_HOST_COMPILER_PATH = ('$GCC_PATH/snos/bin/gcc')

-NVCC_PATH = '%{nvcc_path}'
-PREFIX_DIR = os.path.dirname(GCC_HOST_COMPILER_PATH)
+CURRENT_DIR = os.path.dirname(sys.argv[0])
+NVCC_PATH = ('$CRAY_CUDATOOLKIT_DIR/bin/nvcc')
+LLVM_HOST_COMPILER_PATH = ('$GCC_PATH/snos/bin/gcc')
+AS_PATH = ('/usr/bin/as')
+PREFIX_DIR = os.path.dirname(AS_PATH)
 NVCC_VERSION = '%{cuda_version}'

 def Log(s):
@@ -194,7 +197,8 @@
   srcs = ' '.join(src_files)
   out = ' -o ' + out_file[0]

-  supported_cuda_compute_capabilities = [ %{cuda_compute_capabilities} ]
+  #supported_cuda_compute_capabilities = [ %{cuda_compute_capabilities} ]
+  supported_cuda_compute_capabilities = ["6.0",]
   nvccopts = '-D_FORCE_INLINES '
   for capability in supported_cuda_compute_capabilities:
     capability = capability.replace('.', '')
